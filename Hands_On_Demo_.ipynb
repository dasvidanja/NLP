{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hands On Demo .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlluaGuo1hch5FqoJ6DeQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasvidanja/NLP/blob/master/Hands_On_Demo_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9wvsdwFPWOd",
        "colab_type": "text"
      },
      "source": [
        "In this section we will go over the basic functions of NLP :) \n",
        "\n",
        "* Import all basic packages\n",
        "* Tokenization\n",
        "    - word \n",
        "    - sentence \n",
        "* Stemming \n",
        "\n",
        "* Lemmatization \n",
        "* Stop Words\n",
        "* Part-of-Speech Tagging \n",
        "* Sentiment Analysis \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhwfx2esqJLc",
        "colab_type": "text"
      },
      "source": [
        "Import all of the necessary packages needed to perform basic NLP operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FMHXrUeqS1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "c205287d-1ada-439d-961b-5bbed1cfcb40"
      },
      "source": [
        "#Import of all packages needed for NLP basics\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from textblob import TextBlob\n",
        "import string \n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPtV2WAMFSdU",
        "colab_type": "text"
      },
      "source": [
        "In here we are going to tokenize some text. The first function we is is **sent_tokenize()** which takes one argument the text that we want to tokenize. This function allows us to split a deteminate text into sentences. Your job is to apply a similar function to the text, where the goal is to split the text into words. The function that allows for such task to take place is **word_tokenize()** dont't forget to pass (insert in parenthesis) the text that we want to tokenize. Also print the results at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MkM-jC635C7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "76933044-1d23-422d-984f-2ea3906c9aa1"
      },
      "source": [
        "#Text \n",
        "corpora= \"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence.\" +\\\n",
        "\"It concerned with the interactions between computers and human (natural) languages. In particular how to program computers to process and analyze \"+\\\n",
        "\"large amounts of natural language data. NLP is quite an interesting field. One can do soo much with few functions. NLP is the ultimate frontier.\"\n",
        "\n",
        "#Check out our corpora (text) info\n",
        "print(corpora)\n",
        "print(\"Length: \", len(corpora))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence.It concerned with the interactions between computers and human (natural) languages. In particular how to program computers to process and analyze large amounts of natural language data. NLP is quite an interesting field. One can do soo much with few functions. NLP is the ultimate frontier.\n",
            "Length:  425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM_Bz5rz9jCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd96f357-7e12-4391-e36c-fee7e8cf444c"
      },
      "source": [
        "#Tokenization: splitting the data into tokens (tokens can be defined as sentences or words)\n",
        "#Sentence tokens\n",
        "sentence_token =  sent_tokenize(corpora)\n",
        "\n",
        "#Find out the type object returned by function sent_tokenize\n",
        "print(type(sentence_token))\n",
        "\n",
        "#print all elements of sentence_token with its index starting frome one\n",
        "for index in range(len(sentence_token)):\n",
        "  print(\"Token\",index+1,': ', sentence_token[index])\n",
        "\n",
        "##HAVE STUDENTS FILL THIS PORTION AND REFER TO THE SENTENCE CODE TO  COME UP WITH THE WORD CODE?\n",
        "#Word tokens\n",
        "word_token =  word_tokenize(corpora)\n",
        "\n",
        "#Find out the type object returned by function sent_tokenize\n",
        "print(type(word_token))\n",
        "\n",
        "#print all elements of word_token with its index starting frome one\n",
        "for index in range(len(word_token)):\n",
        "  print('Token',index,': ', word_token[index])\n",
        "\n",
        "#Summary of results\n",
        "sent_len = len(sentence_token)\n",
        "word_len = len(word_token)\n",
        "\n",
        "print('There are ',sent_len, ' total sentences in the text.')\n",
        "print('There are ', word_len, ' total words in the text.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "Token 1 :  Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence.It concerned with the interactions between computers and human (natural) languages.\n",
            "Token 2 :  In particular how to program computers to process and analyze large amounts of natural language data.\n",
            "Token 3 :  NLP is quite an interesting field.\n",
            "Token 4 :  One can do soo much with few functions.\n",
            "Token 5 :  NLP is the ultimate frontier.\n",
            "<class 'list'>\n",
            "Token 0 :  Natural\n",
            "Token 1 :  language\n",
            "Token 2 :  processing\n",
            "Token 3 :  (\n",
            "Token 4 :  NLP\n",
            "Token 5 :  )\n",
            "Token 6 :  is\n",
            "Token 7 :  a\n",
            "Token 8 :  subfield\n",
            "Token 9 :  of\n",
            "Token 10 :  linguistics\n",
            "Token 11 :  ,\n",
            "Token 12 :  computer\n",
            "Token 13 :  science\n",
            "Token 14 :  ,\n",
            "Token 15 :  information\n",
            "Token 16 :  engineering\n",
            "Token 17 :  ,\n",
            "Token 18 :  and\n",
            "Token 19 :  artificial\n",
            "Token 20 :  intelligence.It\n",
            "Token 21 :  concerned\n",
            "Token 22 :  with\n",
            "Token 23 :  the\n",
            "Token 24 :  interactions\n",
            "Token 25 :  between\n",
            "Token 26 :  computers\n",
            "Token 27 :  and\n",
            "Token 28 :  human\n",
            "Token 29 :  (\n",
            "Token 30 :  natural\n",
            "Token 31 :  )\n",
            "Token 32 :  languages\n",
            "Token 33 :  .\n",
            "Token 34 :  In\n",
            "Token 35 :  particular\n",
            "Token 36 :  how\n",
            "Token 37 :  to\n",
            "Token 38 :  program\n",
            "Token 39 :  computers\n",
            "Token 40 :  to\n",
            "Token 41 :  process\n",
            "Token 42 :  and\n",
            "Token 43 :  analyze\n",
            "Token 44 :  large\n",
            "Token 45 :  amounts\n",
            "Token 46 :  of\n",
            "Token 47 :  natural\n",
            "Token 48 :  language\n",
            "Token 49 :  data\n",
            "Token 50 :  .\n",
            "Token 51 :  NLP\n",
            "Token 52 :  is\n",
            "Token 53 :  quite\n",
            "Token 54 :  an\n",
            "Token 55 :  interesting\n",
            "Token 56 :  field\n",
            "Token 57 :  .\n",
            "Token 58 :  One\n",
            "Token 59 :  can\n",
            "Token 60 :  do\n",
            "Token 61 :  soo\n",
            "Token 62 :  much\n",
            "Token 63 :  with\n",
            "Token 64 :  few\n",
            "Token 65 :  functions\n",
            "Token 66 :  .\n",
            "Token 67 :  NLP\n",
            "Token 68 :  is\n",
            "Token 69 :  the\n",
            "Token 70 :  ultimate\n",
            "Token 71 :  frontier\n",
            "Token 72 :  .\n",
            "There are  5  total sentences in the text.\n",
            "There are  73  total words in the text.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbNVIwaCqYoZ",
        "colab_type": "text"
      },
      "source": [
        "In "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05dtKwh59mEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "f8c98c59-7e65-4827-9e7b-a0cb397d52f8"
      },
      "source": [
        "#Stemming: removing affixes (‘extras’) from a word and reduce it to its basic root.\n",
        "\n",
        "#Create stemmer object\n",
        "stemmer = PorterStemmer() \n",
        "\n",
        "#Define a word\n",
        "single_word= \"Programers\"\n",
        "\n",
        "#Stem word by calling the stemmer object and calling the function stem:  stemmer.stem()\n",
        "stem_single_word = stemmer.stem(single_word)\n",
        "\n",
        "#Print resuls\n",
        "print(\"Word: \",single_word,\" Stemmmed version: \", stem_single_word)\n",
        "\n",
        "\n",
        "#HAVE STUDENTS DO THIS PORTION\n",
        "#Define list with words\n",
        "multiple_words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', 'agreed', 'owned', 'humbled', 'sized','meeting', 'stating', \n",
        "                  'siezing', 'itemization', 'sensational', 'traditional', 'reference', 'colonizer', 'plotted']\n",
        "\n",
        "\n",
        "for x in multiple_words:\n",
        "  print(\"Word: \",x,\" Stemmmed version: \", stemmer.stem(x))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word:  Programers  Stemmmed version:  program\n",
            "Word:  caresses  Stemmmed version:  caress\n",
            "Word:  flies  Stemmmed version:  fli\n",
            "Word:  dies  Stemmmed version:  die\n",
            "Word:  mules  Stemmmed version:  mule\n",
            "Word:  denied  Stemmmed version:  deni\n",
            "Word:  died  Stemmmed version:  die\n",
            "Word:  agreed  Stemmmed version:  agre\n",
            "Word:  owned  Stemmmed version:  own\n",
            "Word:  humbled  Stemmmed version:  humbl\n",
            "Word:  sized  Stemmmed version:  size\n",
            "Word:  meeting  Stemmmed version:  meet\n",
            "Word:  stating  Stemmmed version:  state\n",
            "Word:  siezing  Stemmmed version:  siez\n",
            "Word:  itemization  Stemmmed version:  item\n",
            "Word:  sensational  Stemmmed version:  sensat\n",
            "Word:  traditional  Stemmmed version:  tradit\n",
            "Word:  reference  Stemmmed version:  refer\n",
            "Word:  colonizer  Stemmmed version:  colon\n",
            "Word:  plotted  Stemmmed version:  plot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5XLupxI68rm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "outputId": "9c6f8d93-9a5c-4d8a-ee7b-b57ee482c9f7"
      },
      "source": [
        "#Lemmatization: applying universal meaning to similar words  went → go, best → good\n",
        "#Create a Lemma object\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "#Define a word\n",
        "single_word= \"went\"\n",
        "\n",
        "#Lemmatize word by calling the lemma object and calling the function lemmatize:  lemmatizer.lemmatize()\n",
        "lemma_single_word = lemmatizer.lemmatize(single_word, pos='v')\n",
        "\n",
        "#Print resuls\n",
        "print(\"Word: \",single_word,\" Lemmatized version: \", lemma_single_word)\n",
        "\n",
        "#HAVE STUDENTS DO THIS PART\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "\n",
        "#Step one tokenize the sentence into words\n",
        "token= word_tokenize(sentence)\n",
        "#Check if it is indeed a list made of the words of sentence\n",
        "print(token)\n",
        "\n",
        "#Create list and initialize  with a tuple [('Word','Lemmatized')]\n",
        "lemma_list = [('Word','Lemmatized')]\n",
        "\n",
        "#Apply Lemma function -->  save in a tuple --> append to lemma_list\n",
        "for x in token:\n",
        "  word_lemma = (x,lemmatizer.lemmatize(x, pos='v'))\n",
        "  lemma_list.append(word_lemma)\n",
        "\n",
        "#Print contents of lemma_list\n",
        "for word in lemma_list:\n",
        "  print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word:  went  Lemmatized version:  go\n",
            "['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun', '.']\n",
            "('Word', 'Lemmatized')\n",
            "('He', 'He')\n",
            "('was', 'be')\n",
            "('running', 'run')\n",
            "('and', 'and')\n",
            "('eating', 'eat')\n",
            "('at', 'at')\n",
            "('same', 'same')\n",
            "('time', 'time')\n",
            "('.', '.')\n",
            "('He', 'He')\n",
            "('has', 'have')\n",
            "('bad', 'bad')\n",
            "('habit', 'habit')\n",
            "('of', 'of')\n",
            "('swimming', 'swim')\n",
            "('after', 'after')\n",
            "('playing', 'play')\n",
            "('long', 'long')\n",
            "('hours', 'hours')\n",
            "('in', 'in')\n",
            "('the', 'the')\n",
            "('Sun', 'Sun')\n",
            "('.', '.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFRyZEWwyDAa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "431161d5-548c-45e8-c06f-2ee4c7e380ee"
      },
      "source": [
        "#Stop Words: meaningless words that appear in text but that give/or take no additional meaning from major idea\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Select stop words for English language\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "#Check what the type of the object stop is, its length and contents\n",
        "print(type(stop))\n",
        "print(len(stop))\n",
        "print(stop)\n",
        "\n",
        "#Remove stop words from the text\n",
        "#1 define sentence \n",
        "text = \"Hello friends and family. I am happy to announce today that I will be taking some time off, so as to focus on learning python. I hope you don't mind.\"\n",
        "\n",
        "#2 Tokenize text into indivudal words & first 10 elements\n",
        "token_text = word_tokenize(text)\n",
        "print(token_text[0:11])\n",
        "\n",
        "#3 Remove stopwords ( create a new list --> and all words to the new list that are not present in the stop_word list)\n",
        "stop_free_token_text = []\n",
        "for words in token_text:\n",
        "  if words not in stop:\n",
        "    stop_free_token_text.append(words)\n",
        "\n",
        "#4 Check out the results\n",
        "print(stop_free_token_text)\n",
        "\n",
        "#*To see elements that were discareded\n",
        "diff = [ z for z in token_text if z not in  stop_free_token_text]\n",
        "print(diff)\n",
        "\n",
        "\n",
        "#FOR STUDENTS REMOVE STOP WORDS & PUNCTUATIONS \n",
        "punctuation = string.punctuation\n",
        "print(punctuation)\n",
        "\n",
        "no_punc_no_stop = []\n",
        "for word in token_text:\n",
        "  if word not in stop and word not in punctuation:\n",
        "    no_punc_no_stop.append(word)\n",
        "print(no_punc_no_stop)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['Hello', 'friends', 'and', 'family', '.', 'I', 'am', 'happy', 'to', 'announce', 'today']\n",
            "['Hello', 'friends', 'family', '.', 'I', 'happy', 'announce', 'today', 'I', 'taking', 'time', ',', 'focus', 'learning', 'python', '.', 'I', 'hope', \"n't\", 'mind', '.']\n",
            "['and', 'am', 'to', 'that', 'will', 'be', 'some', 'off', 'so', 'as', 'to', 'on', 'you', 'do']\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "['Hello', 'friends', 'family', 'I', 'happy', 'announce', 'today', 'I', 'taking', 'time', 'focus', 'learning', 'python', 'I', 'hope', \"n't\", 'mind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G59Qv2VTtv08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "051570ea-fc43-4066-be52-7513bcd8d5a6"
      },
      "source": [
        "#Part of Speech Tagger: assign to each word/token grammatical meaning \n",
        "#Define a simple sentence \n",
        "text = 'I am eating an apple'\n",
        "\n",
        "#1  tokenize text into words & print \n",
        "word_text = word_tokenize(text)\n",
        "print(word_text)\n",
        "\n",
        "#2 Apply P-O-S Tagger to tokenized text \n",
        "tagged_text = nltk.pos_tag(word_text)\n",
        "  \n",
        "print(tagged_text) \n",
        "\n",
        "#Check out list of possible taks \n",
        "print(nltk.help.upenn_tagset())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'eating', 'an', 'apple']\n",
            "[('I', 'PRP'), ('am', 'VBP'), ('eating', 'VBG'), ('an', 'DT'), ('apple', 'NN')]\n",
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tpdip5KOhQK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "144b6445-c1c5-4157-fb60-483b635c1de6"
      },
      "source": [
        "#Sentiment Analysis: determining the positivity/negativity of a determinate token (word/sentence)  \n",
        "text = TextBlob(\"Python is horrible! Hello. You are beautiful. I AM NOT SURE. I am very sure. What a day!\")\n",
        "\n",
        "num=1\n",
        "for sentence in text.sentences:\n",
        "  print(num, \" \", sentence)\n",
        "  if(sentence.sentiment.polarity>0):\n",
        "    print ('    Score: positive')\n",
        "  elif(sentence.sentiment.polarity==0):\n",
        "    print('    Score: neutral')\n",
        "  elif(sentence.sentiment.polarity<0):\n",
        "    print('    Score: negative')\n",
        "  print(\"\\n\")\n",
        "  num +=1\n",
        "\n",
        "#FOR STUDENTS\n",
        "#What is the  outcome of the word sentence \"Banana is yellow, but independently of its color it is a great food.\"\n",
        "text = TextBlob(\"Banana is yellow, but independently of its color it is a great food.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1   Python is horrible!\n",
            "    Score: negative\n",
            "\n",
            "\n",
            "2   Hello.\n",
            "    Score: neutral\n",
            "\n",
            "\n",
            "3   You are beautiful.\n",
            "    Score: positive\n",
            "\n",
            "\n",
            "4   I AM NOT SURE.\n",
            "    Score: negative\n",
            "\n",
            "\n",
            "5   I am very sure.\n",
            "    Score: positive\n",
            "\n",
            "\n",
            "6   What a day!\n",
            "    Score: neutral\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.26666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XUtAPSdZOXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d6672f71-f2d6-4ef7-902c-be702dfde08b"
      },
      "source": [
        "good = TextBlob(\"I love you.\")\n",
        "bad = TextBlob(\"I hate you.\")\n",
        "neutral = TextBlob(\"Maybe I like you, maybe I don't\")\n",
        "print(text.polarity)\n",
        "print(bad.polarity)\n",
        "print(neutral.polarity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "-0.8\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}